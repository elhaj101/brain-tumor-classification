{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **(Modeling And Evaluation)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Build and train a custom convolutional neural network (CNN) from scratch for tumor detection in CT scans.\n",
        "* Tune hyperparameters to optimize model performance.\n",
        "* Evaluate the model using accuracy, recall, and inference time metrics.\n",
        "* Generate model predictions and confidence scores for downstream visualization.\n",
        "* Prepare the model and outputs for integration with the Streamlit dashboard.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Preprocessed and augmented image data and metadata from the DataCollection notebook.\n",
        "* Train/validation/test splits.\n",
        "* Any configuration files or parameters for model training.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Trained custom CNN model (saved in a suitable format, e.g., .h5 or .pb).\n",
        "* Evaluation metrics (accuracy, recall, inference time) and confusion matrix.\n",
        "* Model predictions and confidence scores for each sample.\n",
        "* Artifacts for dashboard integration (e.g., prediction results, model files).\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* The model should be compact enough for real-time inference (<1.5 sec/sample).\n",
        "* Early stopping and validation loss monitoring should be used to prevent overfitting.\n",
        "* All outputs will be used in the DataVisualization notebook and Streamlit dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir('/workspaces/brain-tumor-classification')\n",
        "print(\"Current working directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "**Environment Setup, Data loading and preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Core libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {keras.__version__}\")\n",
        "print(f\"Numpy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Loading & Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dir = \"inputs/brain_tumor_dataset/images/train\"\n",
        "val_dir = \"inputs/brain_tumor_dataset/images/val\"\n",
        "test_dir = \"inputs/brain_tumor_dataset/images/test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_images(directory):\n",
        "    total = 0\n",
        "    for label in os.listdir(directory):\n",
        "        class_path = os.path.join(directory, label)\n",
        "        if os.path.isdir(class_path):\n",
        "            total += len([f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "    return total\n",
        "\n",
        "print(\"Train images:\", count_images(train_dir))\n",
        "print(\"Validation images:\", count_images(val_dir))\n",
        "print(\"Test images:\", count_images(test_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Preparation & Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Utility Function for tf.data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 8\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build File Path and Label Lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "def get_file_paths_and_labels(data_dir):\n",
        "    class_names = sorted(os.listdir(data_dir))\n",
        "    file_paths = []\n",
        "    labels = []\n",
        "    for idx, class_name in enumerate(class_names):\n",
        "        class_dir = os.path.join(data_dir, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            files = glob.glob(os.path.join(class_dir, '*'))\n",
        "            file_paths.extend(files)\n",
        "            labels.extend([idx] * len(files))\n",
        "    return file_paths, labels, class_names\n",
        "\n",
        "train_files, train_labels, class_names = get_file_paths_and_labels(train_dir)\n",
        "val_files, val_labels, _ = get_file_paths_and_labels(val_dir)\n",
        "test_files, test_labels, _ = get_file_paths_and_labels(test_dir)\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Train samples:\", len(train_files))\n",
        "print(\"Validation samples:\", len(val_files))\n",
        "print(\"Test samples:\", len(test_files))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "because all images are PNG , this is a result of a debug. (34)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_image(file_path, label):\n",
        "    img = tf.io.read_file(file_path)\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create tf.data Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((train_files, train_labels))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_files, val_labels))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_files, test_labels))\n",
        "\n",
        "train_ds = train_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds = train_ds.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm Class Balance in Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "unique, counts = np.unique(train_labels, return_counts=True)\n",
        "class_balance = dict(zip(class_names, counts))\n",
        "print(\"Class balance in training set:\", class_balance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Architecture Design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_custom_cnn(input_shape=(224, 224, 3)):\n",
        "    model = models.Sequential([\n",
        "        layers.InputLayer(input_shape=input_shape),\n",
        "        layers.Normalization(),  # Input normalization layer\n",
        "\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_custom_cnn()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Compilation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Choose an appropriate optimizer (e.g., Adam), loss function (e.g., binary crossentropy), and evaluation metrics (accuracy, recall).\n",
        "Compile the model with these settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compile the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import optimizers, metrics\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', metrics.Recall()]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run this to check if tensorflow could not be resolved error means tensorflow is not installed in your instance, in this case it is installed and we have no problem ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "print(sys.executable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Recall-Focused Model Training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the model on the training set, validating on the validation set.\n",
        "Use callbacks such as EarlyStopping and ModelCheckpoint, monitoring validation recall.\n",
        "Log training and validation metrics for each epoch.\n",
        "Track the precision-recall tradeoff.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set Up Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_recall', patience=5, mode='max', restore_best_weights=True, verbose=1),\n",
        "    ModelCheckpoint('best_model.keras', monitor='val_recall', mode='max', save_best_only=True, verbose=1)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\".gitignore\", \"a\") as f:\n",
        "    f.write(\"\\n# Ignore model files\\n*.h5\\n*.keras\\nbest_model.h5\\nmy_model.keras\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=30,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(\"my_model.keras\")\n",
        "print(\"Model saved locally as my_model.keras\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "3.12.1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
