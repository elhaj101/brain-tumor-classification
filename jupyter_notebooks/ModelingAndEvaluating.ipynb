{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **(Modeling And Evaluation)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Build and train a custom convolutional neural network (CNN) from scratch for tumor detection in CT scans.\n",
        "* Tune hyperparameters to optimize model performance.\n",
        "* Evaluate the model using accuracy, recall, and inference time metrics.\n",
        "* Generate model predictions and confidence scores for downstream visualization.\n",
        "* Prepare the model and outputs for integration with the Streamlit dashboard.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Preprocessed and augmented image data and metadata from the DataCollection notebook.\n",
        "* Train/validation/test splits.\n",
        "* Any configuration files or parameters for model training.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Trained custom CNN model (saved in a suitable format, e.g., .h5 or .pb).\n",
        "* Evaluation metrics (accuracy, recall, inference time) and confusion matrix.\n",
        "* Model predictions and confidence scores for each sample.\n",
        "* Artifacts for dashboard integration (e.g., prediction results, model files).\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* The model should be compact enough for real-time inference (<1.5 sec/sample).\n",
        "* Early stopping and validation loss monitoring should be used to prevent overfitting.\n",
        "* All outputs will be used in the DataVisualization notebook and Streamlit dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspaces/brain-tumor-classification'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /workspaces/brain-tumor-classification\n"
          ]
        }
      ],
      "source": [
        "os.chdir('/workspaces/brain-tumor-classification')\n",
        "print(\"Current working directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "**Environment Setup, Data loading and preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Core libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "Keras version: 3.10.0\n",
            "Numpy version: 1.26.1\n",
            "Pandas version: 2.1.1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {keras.__version__}\")\n",
        "print(f\"Numpy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Loading & Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dir = \"inputs/brain_tumor_dataset/images/train\"\n",
        "val_dir = \"inputs/brain_tumor_dataset/images/val\"\n",
        "test_dir = \"inputs/brain_tumor_dataset/images/test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train images: 7030\n",
            "Validation images: 1054\n",
            "Test images: 1054\n"
          ]
        }
      ],
      "source": [
        "def count_images(directory):\n",
        "    total = 0\n",
        "    for label in os.listdir(directory):\n",
        "        class_path = os.path.join(directory, label)\n",
        "        if os.path.isdir(class_path):\n",
        "            total += len([f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "    return total\n",
        "\n",
        "print(\"Train images:\", count_images(train_dir))\n",
        "print(\"Validation images:\", count_images(val_dir))\n",
        "print(\"Test images:\", count_images(test_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Preparation & Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Utility Function for tf.data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def preprocess_image(file_path, label):\n",
        "    img = tf.io.read_file(file_path)\n",
        "    img = tf.image.decode_image(img, channels=3)\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = tf.cast(img, tf.float32) / 255.0  # Normalize to [0, 1]\n",
        "    return img, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build File Path and Label Lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes: ['notumor', 'tumor']\n",
            "Train samples: 7030\n",
            "Validation samples: 1054\n",
            "Test samples: 1054\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "\n",
        "def get_file_paths_and_labels(data_dir):\n",
        "    class_names = sorted(os.listdir(data_dir))\n",
        "    file_paths = []\n",
        "    labels = []\n",
        "    for idx, class_name in enumerate(class_names):\n",
        "        class_dir = os.path.join(data_dir, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            files = glob.glob(os.path.join(class_dir, '*'))\n",
        "            file_paths.extend(files)\n",
        "            labels.extend([idx] * len(files))\n",
        "    return file_paths, labels, class_names\n",
        "\n",
        "train_files, train_labels, class_names = get_file_paths_and_labels(train_dir)\n",
        "val_files, val_labels, _ = get_file_paths_and_labels(val_dir)\n",
        "test_files, test_labels, _ = get_file_paths_and_labels(test_dir)\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Train samples:\", len(train_files))\n",
        "print(\"Validation samples:\", len(val_files))\n",
        "print(\"Test samples:\", len(test_files))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "because all images are PNG , this is a result of a debug. (34)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_image(file_path, label):\n",
        "    img = tf.io.read_file(file_path)\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create tf.data Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((train_files, train_labels))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_files, val_labels))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_files, test_labels))\n",
        "\n",
        "train_ds = train_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds = train_ds.shuffle(buffer_size=1000).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm Class Balance in Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class balance in training set: {'notumor': 3515, 'tumor': 3515}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "unique, counts = np.unique(train_labels, return_counts=True)\n",
        "class_balance = dict(zip(class_names, counts))\n",
        "print(\"Class balance in training set:\", class_balance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Architecture Design"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "3.12.1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
